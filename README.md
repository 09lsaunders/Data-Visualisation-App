# Data-Visualisation-App
# Overview
This is a web application developed during my final year at university.
The app is split into an asynchronous database update/creation file ('Async Database Creation 4.py') while the application itself is ran using a flask framework ('app.py').

Please note only small amounts of data have been uploaded as dummy datasets, thereofre data is limited to the first few days/first week of November 2018. No data will be found at any other dates
# Async File
The Async file uses pandas to read csv files of data (ems-nov-2018_for_upload.csv, synetica-nov-2018_for_upload.csv, wifi_2018-11_for_upload.csv), locates the relavant data columns and attributes, it also created a SQLite database for each datatype - in this case EMS (energy management system), synteica and wifi data. It then finds all instances of data variables in most cases using an ID or unique name and assigns these to an array, this array is then looped through with data located for each unique ID - data is stored as a dataframe consisting of the data and dates and timestamps. A table name is then assigned to this data and  is created in the SQLite database with associated data assigned to this table - during the loop each table name is printed to let the user see how far through the datasets the code is and ensures a user can see if th ecode gets stuck.

At the end of each loop a test query is completed with the time axis from this table printed to ensure data assignment has been successful. A list of the unique table names created during the code is also output to a csv file.
# Synchronous File app.py
The application itself is based on the flask microframework, the code can be used in production mode or development mode by selecting to comment out the sys.stdout variable. The code has been designed in a way that a lot of the functionality is duplicated for each dataset imported allowing to easy addtions of additonal datasets.

Initially the code will go to the template 'view 19 index.html', the only option here is to select data to be loaded. Upon submission of the form 'app.py' recieves the value of the button pressed so the code knows that the submit button was pressed and not other buttons which are available once data is trended. this is fed into an if statement to show the load button was pressed (this is the value assigned to the submit buttton).
# Loading Data
The selected dataset is assigned to the datatype variable. Using EMS loading as an example, an array of titles is assigned to the meter ID in an array used later to assign the title passed to the template. Next, from the dropdown menu important values for querying the database such as the meter ID itself, the start and end dates selected, and the location selected. if statements are inserted in case a field is left empty. The values are then formatted for SQL query statements. The unique table names csv file created by the async program is then imported and is looped through to check the table queried actually exists, if it does the database is queried otherwise variables are assigned to empty arrays. The queried data and timestamps are assigned to a pandas dataframe. Once data is queried a model is created of the data using a time period 4 times greater than the loaded time period with weekdays and weekends dates noted and stored in weekend and weekday arrays. data is also split at this point into weekend and weekdays and assigned to variables for use in the modelling function. Data is then queried for the weekend model period and weekday model dates and all data is appended to an array, if statements ensure if no weekend data is found of the model period does not include a weekend or weekday the arrays are assigned as empty. Weekday and weekend values are then averaged using 2 hours’ worth of data and using scikit learn make_pipeline and regression modelling functionality a model is created predicting that the data requested is calculated to be. Titles are then assigned using the array assigned at the start, values and timestamp labels are also formatted at this point ready for passing to the template. The data is looped through and using standard deviations of loaded data if no model was created, otherwise if a model exists the model is used and if the value is 1 standard deviation away from the model then it is assigned to an outlier array – this is essentially highlighted in the final trend- if not an outlier then an ‘f’ is assigned, this is as strings are ignored by the chart.js library used to trend data while allowing for outliers to remain the same length as the array of data values. Outliers based on models are compared to the correct model based on day of the week i.e., weekend models are used for values on Saturday and Sunday and are compared to the weekday model otherwise. Finally, after this dataframes of values, and outliers and well as max and min values of data as well as the meter ID loaded are written to csv files for later use.  The variables needed for loading the data and assigning legends and titles are passed to the template ‘view 19.html’, an if statement checks that data was found and if not loads the index again with a title containing an error message.

This algorithm is a base for almost all data querying even adding data.
# Adding Additional Data
When ‘view 19.html’ is loaded successfully data should be visible with outliers also highlighted, a button next to submit call add is now available, an axis value changing option should be visable and an additional dropdown menu below the graph should be visible. Selecting a different variable and selecting add will add the selected data to the graph already loaded up to 3 trends can be loaded before the oldest trend disappears.
Again, using adding EMS data as an example, the code will go to the newly created residual csv files and load values and timestamps and title, etc from these and assign the to variables. Be aware the code is based on windows, when ported to linux the csv files are written with columns in reverse order, so swapping the ‘.iloc’ values is required. From here the process is the same as loading with slight additions. First additional code is retrieving residual values from all csv files and assigning them to variables starting with newest, these are stored in case an error occurs such as no data available or data is already trended. Next data is resampled to 1 minute frequency to ensure timestamps are all on the same frequency, this also allows for 2 different start and end dates to be chosen as the resampling is between the earliest start dat and latest end date – 1 minute is chosen as the wifi data is sampled in frequently and can have strange timestamps like 12:04:36 – and so datetime labels do not contain seconds. After resampling the data is written to csv files the same as before, the biggest change is the if statements to check for errors. The first set checks for no data found and hen subsequently checks whether 1, 2, or 3 data trends were loaded before the latest add request and writes the previously saved variables to csv files to essentially reset the csv files to what they contained previously. The next set of more complicated if statements check if data is already loaded and again reset the residual csv files.

In cases where data is already trended the submit button also acts as a clear and load button.
# Adjusting Axis Values
Below the load and add button is an input box for axis values where the axis values can be changed from their defaults to make data viewing easier. The button(s) below the inputs are related to which axis values to change - the axis’ are colour coded and correspond with the same colour trend – inputting values and selecting the data axis to change will activate an algorithm in the code based on which html template is being used (‘view 19.html’, ‘view 19 2 lines.html’, ‘view 19 3 lines.html’) and whether the data is the latest added value, the 2nd oldest, or the oldest data trended. Again, no matter the combination of line and template; the algorithm first reads the corresponding residual data files to the button pressed ‘Residualdataadjust.csv’ for the latest trended data, ‘Residualdataadjust2.csv’ for the 2nd oldest trend and ‘Residualdataadjust3.csv’ for the oldest dataset trended. Data, timestamps and title are extracted from the dataframe imported and then a maximum and minimum value are calculated for the existing data, this is so if the inputs are blank a reset value for the axis maximum and minimum can be used. From here values from the inputs are extracted by the algorithm and are checked for validity i.e., checked to see if they are blank. Outlier data is read from the corresponding file and residual start and end dates are calculated to be input into the default values of the date selection on the data selection menu.
# Finding The Closest Correlated Data
The dropdown on the ‘view 19.html’ template below the axis will show a dropdown with only 1 option for ‘select comparison type’ and compare. The first dropdown shows ‘Find closest data correlation’ and the second will show your loaded data title. The 3rd dropdown has options for the 3 datatypes: EMS, Synetica and Wifi, and all data options. The option in the top dropdown field is a selected algorithm for finding the closest possible data to the current loaded data. This works by loading in all data and comparing the titles of each residual file to the title of the selected data to be compared. Once identified variables are assigned to the exacted data from the correct file, from here the values of the data are formatted to contain ‘not a number’ variables where strings are located, and the original values are saved to a different variable for later use. The not a number data is replaced by front filling and then back filling to ensure fair comparisons. All unique tables are then loaded from the corresponding data tables csv file or all 3 if all data is selected. These tables are then looped through with data queried for each, resampled to 1 minute data frequency and compared to the original data with an r value created from pandas ‘.corr’ function. The r value is then compared to a variable containing the highest r value so far and if the r value contains the highest positive correlation the details and data and r value are then saved to permanent global variables, there is also a check before this to ensure the titles do not match as r would be 1 for identical data and the algorithm would always give the compared variable. After each table is looped though the same model creation and outlier detection code is used to calculate the outliers for the closest values as is used in loading and adding of data – the only difference is when all data is selected and the dataset that the closest values belong to is found by comparing the ID of this data to the unique table names from the array assigned earlier when pulling in the unique table names csv file. Once complete data is resampled as with adding any new data to the trend, then any remaining nan (not a number) values are replaced by the string ‘f’ and maximum and minimum values are calculated for the axis values. Finally, data added though the closest correlation algorithm is written to csv files so the closest value data is not lost if the user wants to add more data after running the algorithm.
# Correlating User Selected
Lastly, if 2 trends are added to the chart where previously the dropdown below the graph axis only had 1 option there will now be 2 ‘select comparison type’ dropdown options. The new option will be call ‘compare data’ and the compare boxes below will feature the titles of any trended data and the second dropdown will contain the title of the data not selected. If this option is selected an algorithm will compare the 2 selected trends. This works by again comparing selected titles to the corresponding titles in saved csv residual files and assigning the variables (title, values outliers, etc) based on which data was added to the trend first, the values are then forward filled and back filled and then averaged over 2 hours to allow for better correlation. From here values are correlated using pandas .corr function, the R value is the saved and added to a trendline of its own on axis of -1 minimum to +1 maximum, a good correlation is around 0.8+ in my experience so far.
# Templates & JavaScript (Chart.js)
The templates contain the CSS and the vanilla JavaScript used to make the dynamic dropdown menus. The CSS itself was found online as a template and used throughout development. The JavaScript also imports the Chart.js library, many libraries are imported however these are just legacy code and the latest Chart.js version should be used. The dynamic dropdown menu uses the ‘onchange’ activation type for functions which change different parts of the dropdown. Each function works the same way, firstly by getting the element ID of the parts of the dropdown to be changed during the function and clearing these to empty them. Arrays of the new options and the HTML value associated with them are with each element set out as ‘value|innerHTML’, the algorithm then loops through each array and splits the elements before assigning the value and innerHTML to a variable which in turn is added to the dropdown menu. 

Next is the Chart.js portion of the script which involves formatting the timestamps, values and outliers to an array format accepted by chart.js. From here the chart canvas which is named in the main HTML section is selected and formatted. First the type of chart is defined (type is line for this application), next data is added with the later defined y axis, the legend, colours and point radius all defined and customised. Important to note is the span gaps being set to true, this means the code ignores strings and joins all numbers in the array of data. After data is added the x axis and y axes are customised under the ‘scales’ section. Only 1 x-axis exists, and this is set to display ticks only each day and the axis label is added as ‘Date/Time’. The y axes are added next with 1 axis added per dataset trended and the name of the template describes how many trends are set up on the template. The y axis name is defined, and below this the max and min values, axis labels, colous and position are defined as well as type being linear.
